---
layout: post
mathjax: true
title: "Entropy"
author:
- Andrei Radulescu-Banu
---
References:
* C. Shannon: [A Mathematical Theory of Communication](http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) (1948)
* E.T. Jaynes: [Probability Theory: The Logic of Science](https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712) (2003)
* K. Mallick: [Thermodynamics and Information Theory](http://www.bourbaphy.fr/mallick.pdf), [video](https://www.youtube.com/watch?v=pXyONXaqqP8)
* O. Rioul: [This is IT: A Primer on Shannonâ€™s Entropy and Information](http://www.bourbaphy.fr/rioul.pdf), [video](https://www.youtube.com/watch?v=vinCEpee-tc)
* [The Cross-Entropy Method](https://www.amazon.com/Cross-Entropy-Method-Combinatorial-Optimization-Monte-Carlo/dp/038721240X), R. Rubinstein, D. P. Kroese (2004)
* [Towards Data Science](https://towardsdatascience.com): [Cross-entropy method for Reinforcement Learning](https://towardsdatascience.com/cross-entropy-method-for-reinforcement-learning-2b6de2a4f3a0), A.. Khare (2020)
* Wikipedia [entropy](https://en.wikipedia.org/wiki/Entropy) and [information entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) pages

Suppose we start with a discrete probability distribution $(p_1, p_2, ..., p_n)$, where $$0 \le p_i \le 1$$, and $$\sum p_i = 1$$. How would we define the entropy, or the amount of information $$H(p_1, ..., p_n)$$ given by the probability distribution?

We can reason by analogy with thermodynamics, see [Mallick](http://www.bourbaphy.fr/mallick.pdf), where _entropy of a microcanonical system enumerates the total number of microscopic states_, $$\Omega(E, V)$$, _and the Bolzmann formula states that entropy is_

$$
\begin{align*}
S = k_B log \Omega \,\,\, \mathrm{with} \,\,\, k_B \sim 1.3810^{-23}
\end{align*}
$$

If $$p_i$$ are all rational numbers, we can write them as $$p_i = \frac{a_i}{N}$$, with $$a_i, N$$ nonnegative integers, and we can define by analogy the combinatorial entropy of $$a_1, ... a_n$$ as: 

$$
\begin{align*}
H_N(a_1, ..., a_n) = \frac{1}{N} \ln {N \choose a_1, ... a_n} = \frac{1}{N} \ln \frac{N!}{a_1! ... a_n!}
\end{align*}
$$

Here, the choice of $$N$$ is not unique; we can replace $$N$$ with any of its multiples $$kN$$, and $$a_1, ..., a_n$$ with $$ka_1, ..., ka_n$$. We define

$$
\begin{align*}
H(p_1, ..., p_n) := \underset{k \rightarrow \infty}{\lim} H_{kN}(ka_1, ..., ka_n)
\end{align*}
$$

which evaluates, by the Stirling approximation $$\ln t! \sim t \ln t - t + O(\ln t)$$ to

$$
\begin{align*}
H(p_1, ..., p_n) = \underset{k \rightarrow \infty}{\lim} \frac{1}{kN} (kN \, \ln kN - \sum_{i=1}^n k a_i \ln k a_i) = - \sum_{i=1}^n  \frac{a_i}{N} \, \ln \frac{a_i}{N} = - \sum_{i=1}^n p_i \ln p_i
\end{align*}
$$

To be continued...

<!--

Or, if $$a_1, ... a_n$$ are nonnegative integers, we can define

$$
\begin{align*}
H'(a_1, ..., a_n) = \frac{1}{a_1 + ... + a_n} {a_1 + ... + a_n \choose a_1, ... a_n} = \frac{1}{a_1 + ... + a_n} \frac{(a_1 + ... + a_n)!}{a_1! ... a_n!}
\end{align*}
$$

Thus, $$H'(a_1, ..., a_n)$$ represents the number of ways we can partition a set with $$a_1 +  ... + a_n$$ elements into subsets with $$a_1, ... a_n$$ elements, divided by $$a_1 +  ... + a_n$$. With this definition
* $$H'(a_1, ..., a_n)$$ is symmetric in $$a_1, ... a_n$$
* $$H'(a_1, ..., a_n) = H'(a_1, ..., a_{n-2}, a_{n-1} + a_n) + \frac{a_{n-1} + a_n}{a_1 + ... + a_n} H'(\frac{a_{n-1}}{a_{n-1} + a_n}, \frac{a_{n}}{a_{n-1} + a_n})$$

-->