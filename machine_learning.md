---
layout: page
mathjax: true
title: Machine Learning
---
#### Books, Lecture Notes
* Michael Nielsen: [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
* [Artificial Intelligence: A Modern Approach](repository.unimal.ac.id/1022/1/Artificial%20Intelligence%20-%20A%20Modern%20Approach%203rd%20Ed%20-%20Stuart%20Russell%20and%20Peter%20Norvig%2C%20Berkeley%20%282010%29.pdf), by Russell and Norvig (4th edition, 2020)
* [Deep learning theory lecture notes](https://mjt.cs.illinois.edu/dlt/), Matus Telgarsky
* [Fundamentals of Machine Learning for Predictive Data Analytics](https://www.amazon.com/Fundamentals-Machine-Learning-Predictive-Analytics/dp/0262044692/ref=asc_df_0262044692/), J.D. Kelleher et al (2020)
* [Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond](https://www.amazon.com/Learning-Kernels-Regularization-Optimization-Computation/dp/0262194759), B. Scholkopf, A. Smola (2001)
* [Bio-Inspired Artificial Intelligence: Theories, Methods, and Technologies](https://www.amazon.com/Bio-Inspired-Artificial-Intelligence-Technologies-Intelligent/dp/0262062712), D. Floreano, C. Mattiussi (2008)
* [Deep Learning](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/), I. Goodfellow, Y. Bengio, A. Courville (2016)
* [Deep Learning Systems: Algorithms, Compilers, and Processors for Large-Scale Production](https://deeplearningsystems.ai/), Andres Rodriguez (2020)
* [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/14920326461): Concepts, Tools, and Techniques to Build Intelligent Systems, by Aurelien Geron (3rd ed, 2022) ([Jupyter notebooks](https://github.com/ageron/handson-ml2))
* [Python Machine Learning Cookbook](https://www.amazon.com/Machine-Learning-Python-Cookbook-Preprocessing/dp/1491989386/ref=asc_df_1491989386/), Chris Albon (2018)
* [Learning Machine Learning](https://www.informit.com/store/learning-deep-learning-theory-and-practice-of-neural-9780137470358), M. Ekman (2021), [github](https://github.com/NVDLI/LDL/tree/main/pt_framework)
* [fast.ai](https://www.fast.ai/), [Deep Learning for Coders with Fastai and PyTorch](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527), J. Howard and S. Gugger (2020)
* [Advanced Applied Deep Learning](https://www.amazon.com/Advanced-Applied-Deep-Learning-Convolutional/dp/1484249755), CNNs and Object Detection, by U. Michelucci
* [Python Deep Learning](https://www.amazon.com/Python-Deep-Learning-techniques-architectures-ebook/dp/B07KQ29CQ3), Exploring deep learning techniques and neural network architectures with PyTorch, Keras, and TensorFlow, I. Vasiliev el al (2019), [github](https://github.com/ivan-vasilev/Python-Deep-Learning-SE)
* M. Kochenderfer et al: [Algorithms for Decision Making](https://algorithmsbook.com/files/dm.pdf) (2022)
  * IJCAI keynote talk: [Automated Decision Making for Safety Critical Applications](https://www.youtube.com/watch?v=9b4jryW1JtA) (2021)
* A. Zheng, A. Casari: [Feature Engineering](https://www.amazon.com/Feature-Engineering-Machine-Learning-Principles/dp/1491953241/) (2018)
* S. Raschka et al: [Machine Learning with PyTorch and Scikit-Learn](https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319), [github](https://github.com/rasbt/machine-learning-book), [Andrei's fork](https://github.com/andrei-radulescu-banu/machine-learning-book)
* Jake VanderPlas: [Python Data Science Handbook](https://www.oreilly.com/library/view/python-data-science/9781491912126/), [colab](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb)

#### Books - ML from the Probabilistic Perspective
* [Pattern Recognition and Machine Learning](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738), C. Bishop, [pdf](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)
* Kevin P. Murphy
  * [Machine Learning: A Probabilistic Perspective](https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020), Kevin Murphy (2012)
  * [Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html) (2022)
  * [Probabilistic Machine Learning: Advanced Topics](https://probml.github.io/pml-book/book2.html) (2023)
* [Statistical Learning Theory](https://www.amazon.com/STATISTICAL-LEARNING-THEORY-Vladimir-Vapnik/dp/8126528923), Vladimir Vapnik (1998)
* [The Nature of Statistical Learning Theory](https://www.amazon.com/Nature-Statistical-Learning-Theory/dp/0387945598), V. Vapnik(1998)
* [Bayesian Networks and Decision Graphs](https://www.amazon.com/gp/product/0387682813), T.D. Nielsen, F.V. Jensen (2007)

#### Courses
* P. Abbeel: [Foundations of Deep RL in 6 Lectures](https://www.youtube.com/watch?v=2GwBez0D20A&list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0) (2021), [slides](https://www.dropbox.com/s/f9xyrdkpqugtrvq/l1-mdps-exact-methods.pdf?dl=0)
* University of Amsterdam: [UVA Deep Learning Course](https://uvadlc.github.io/), [UVA Deep Learning Tutorials](https://uvadlc-notebooks.readthedocs.io/en/latest/index.html)
* Yann LeCun
  * [Deep Learning](https://www.youtube.com/watch?v=ChLEJA6J2b8&list=PL80I41oVxglJ0kTDV7i3aHBIXe65nTxE7&index=6) course at College de France (2016)
  * [Deep Learning Course at CDS](https://cds.nyu.edu/deep-learning/), [Andrei's notes](machine_learning/yann_lecun_cds.md)
* CMU
  * [Advanced NLP 2022](https://www.youtube.com/playlist?list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK7z)
  * 10-704: [Information Processing and Learning](https://www.cs.cmu.edu/~aarti/Class/10704/) (Spring 2012)
* G. Hinton: [Neural Networks for Machine Learning](https://www.cs.toronto.edu/~hinton/coursera_lectures.html) (2012)
* G. Hulten: [Machine Learning Course](https://www.youtube.com/watch?v=jxWbJz49HCg&list=PLrQmbzbRJ5mwDinvDEJ5B-KDZlPM-sCYO) (2021). Andrei's [notes](machine_learning/geoff_hulten_machine_learning_course.md).
* Berkeley
  * [CS287-FA19](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/): Advanced Robotics (2020), [youtube](https://www.youtube.com/watch?v=xWPViQ6LI-Q&list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF), P. Abbeel
  * [CS294-158-SP20](https://sites.google.com/view/berkeley-cs294-158-sp20/home): Deep Unsuperviser Learning (Spring 2020), [youtube](https://www.youtube.com/watch?v=V9Roouqfu-M&list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP&index=1) ([Spring 2019](https://sites.google.com/view/berkeley-cs294-158-sp19/home))
* Cornell
  * Volodymyr Kuleshov: CS 5787: [Applied Machine Learning](https://www.youtube.com/playlist?list=PL2UML_KCiC0UlY7iCQDSiGDMovaupqc83), [github](https://github.com/kuleshov/cornell-cs5785-2020-applied-ml) (2020)
* DeepMind: [David Silver: Introduction to Reinforcement Learning](https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver)
* FastAI: [Practical Deep Learning for Coders](https://course.fast.ai/), Jeremy Howard et al.
* Hugo Larochelle: math heavy [Neural networks class](https://www.youtube.com/watch?v=SGZ6BttHMPw&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH), Université de Sherbrooke]
* MIT
  * 6.036 Artificial Intelligence: Patrick Winston Fall 2010 [videos](https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi), Tamara Broderick Fall 2020 [videos](https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi), [slides](https://tamarabroderick.com/ml.html)
  * 6.S191 Introduction to Deep Learning, [IAP 2022](http://introtodeeplearning.com/) [upcoming videos](https://www.youtube.com/channel/UCtslD4DGH6PKyG_1gFAX7sg)
  * 9.520/6.860 Statistical Learning Theory and Applications, [Fall 2019](http://www.mit.edu/~9.520/fall19/), [Fall 2021](https://cbmm.mit.edu/9-520), [videos](https://cbmm.mit.edu/9-520/videos)
* Oxford
  * [Deep Learning for Natural Language Processing](http://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/) (2016-2017)
* Sebastian Raschka:
  * [Introduction to Machine Learning](https://sebastianraschka.com/blog/2021/ml-course.html) - Tree-based Methods, Model Evaluation, and Feature Selection
  * [Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html), 170 Video Lectures from Adaptive Linear Neurons to Zero-shot Classification with Transformers
  * Github: [stat453-deep-learning-ss21](https://github.com/rasbt/stat453-deep-learning-ss21), [deeplearning-models](https://github.com/rasbt/deeplearning-models), [Andrei's fork](https://github.com/andrei-radulescu-banu/deeplearning-models)
* [Stanford](https://www.youtube.com/user/stanfordonline/playlists)
  * [CS221](https://stanford-cs221.github.io): Artificial Intelligence: Principles and Techniques (Autumn 2019), [syllabus](https://stanford-cs221.github.io/autumn2019/),[video](https://www.youtube.com/watch?v=J8Eh7RqggsU&list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX)
  * CS229 - Machine Learning, Autumn 2018 [video](https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) , [slides](http://cs229.stanford.edu/syllabus-autumn2018.html), Summer 2021 [video, slides](http://cs229.stanford.edu/syllabus-summer2019.html), Andrei's [notes](machine_learning/cs229_anand_avati_machine_learning_summer_2019.md)
  * [CS231n](http://cs231n.stanford.edu/): Convolutional Neural Networks for Visual Recognition (Spring 2017), [syllabus](https://cs231n.github.io/), [2016 video](https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC), [2017 videos](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv), [2017 slides](http://cs231n.stanford.edu/slides/2017), [2021 slides](http://cs231n.stanford.edu/slides/2021)
    * Karpathy's [ConvNetJS CIFAR-10 demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)
  * [CS236](https://deepgenerativemodels.github.io/): Deep Generative Models (slides only)
  * [CS224n](http://web.stanford.edu/class/cs224n/):Natural Language Processing with Deep Learning [Winter 2017 video](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6), [Winter 2019 video](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z), [Winter 2021 video](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)
  * [CS224W](http://web.stanford.edu/class/cs224w): Machine Learning with Graphs [Spring 2021 video](https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)
  * C. Huen: [A survivor's guide to AI courses at Stanford](https://huyenchip.com/2018/03/30/guide-to-Artificial-Intelligence-Stanford.html) (2020)

#### Conferences
* [CVPR '20](https://cvpr2020.thecvf.com/)
* [ICLR](https://iclr.cc/)
* [ICML](https://icml.cc/)
* [NAACL '21](https://2021.naacl.org/)
* [NeurIPS](https://nips.cc/)
* [OpML '20](https://www.usenix.org/conference/opml20/conference-program)

#### MLSys
* [Stanford MLSys Seminars](https://www.youtube.com/channel/UCzz6ructab1U44QPI3HpZEQ/videos)
* [MLSys2021](https://slideslive.com/mlsys-2021)

#### Paper Archives
* [arxiv](https://arxiv.org/), [arxiv-sanity](http://www.arxiv-sanity.com), [arxiv-sanity-lite](https://arxiv-sanity-lite.com)
* [OpenReview](https://openreview.net/)

#### Tools
* [Jax](machine_learning/jax.md)
* [PyTorch](machine_learning/pytorch.md)
* [scikit-learn](machine_learning/scikit-learn.md)
* [Spark.ml and Apache Ignite ML](machine_learning/spark.md)
* [TensorFlow](machine_learning/tensorflow.md)
* Others: Caffe(Berkeley), Caffe2(Facebook), MXNet(Amazon), CNTK(Microsoft), Paddle(Baidu)
* FBLearner Flow
  * [Introducing FBLearner Flow: Facebook’s AI backbone](https://engineering.fb.com/2016/05/09/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/)

#### GPUs
See [GPUs](gpus.md) page

#### Tutorials
* Robbie Allen: [Over 200 of the Best Machine Learning, NLP, and Python Tutorials](https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc) (2018)

#### Videos
* 3Blue1Brown Series
  * [What is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)
  * [Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w)
  * [What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
  * [Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)
* sentdex: Deep Learning and Neural Networks with Python and Pytorch
  * [Intro](https://www.youtube.com/watch?v=BzcBsTou0C0)
  * [Data](https://www.youtube.com/watch?v=i2yPxY2rOzs)
  * [Building our neural Network](https://www.youtube.com/watch?v=ixathu7U-LQ)
  * [Training model](https://www.youtube.com/watch?v=9j-_dOze4IM)
  * ...
* Welch Labs
  * [Learning To See](https://www.youtube.com/watch?v=i8D90DkCLhI)
  * [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs)
* MIT 6.5191 [Introduction to Deep Learning](https://www.youtube.com/watch?v=njKP3FqW3Sk&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) (2020)
  * [Convolutional Neural Networks](https://www.youtube.com/watch?v=iaSUYvmCekI), Alexander Amini
  * [Deep Generative Modeling](https://www.youtube.com/watch?v=rZufA635dq4&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=4), Ava Soleimany
  * [Deep Reinforcement Learning](https://www.youtube.com/watch?v=i6Mi2_QM3rA), Alexander Amini
* J. Frankle, M. Carbin:[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) (2019)
* G. Hinton: [What is wrong with convolutional neural nets ?](https://www.youtube.com/watch?v=rTawFwUvnLE) (2017)
* G. Hinton: [Artificial Intelligence: Turning our understanding of the mind right side up](https://www.youtube.com/watch?v=fDR1I2Shw_E) (2017)
* Y. LeCun: [The Epistemology of Deep Learning](https://www.youtube.com/watch?v=gG5NCkMerHU), IAS (2019)
* [Normalized Nerd](https://www.youtube.com/c/NormalizedNerd)
  * [Markov Chains Clearly Explained! Part - 1](https://youtu.be/i3AkTO9HLXo)

#### Interviews
* Lex Fridman
  * [Whisper captions](https://karpathy.ai/lexicap/) by Andrej Karpathy
  * [Ian Goodfellow: Generative Adversarial Networks (GANs)](https://www.youtube.com/watch?v=Z6rxFNMGdn0&t=979s), Lex Fridman Podcast #19 (2019)
  * [Yann LeCun: Deep Learning, ConvNets, and Self-Supervised Learning](https://www.youtube.com/watch?v=SGSOCuByo24), Lex Firdman Podcast #36 (2019)
  * [Stephen Wolfram: Cellular Automata, Computation, and Physics](https://www.youtube.com/watch?v=ez773teNFYA&t=2539s), Lex Fridman Podcast #89 (2020)
  * [David Silver: AlphaGo, AlphaZero, and Deep Reinforcement Learning](https://www.youtube.com/watch?v=uPUEq8d73JI&t=2499s), Lex Fridman Podcast #86 (2020)
  * [Andrew Ng: Deep Learning, Education, and Real-World AI](https://www.youtube.com/watch?v=0jspaMLxBig), Lex Fridman Podcast #73 (2020)
  * [Yann LeCun: Dark Matter of Intelligence and Self-Supervised Learning](https://www.youtube.com/watch?v=SGzMElJ11Cc), Lex Fridman Podcast #258 (2022)
  * [Demis Hassabis: DeepMind - AI, Superintelligence & the Future of Humanity](https://www.youtube.com/watch?v=Gfr50f6ZBvo) (2022)

#### Linear Discriminant Analysis (LDA)
* G. Chen: Math 253: Mathematical Methods for Data Visualization: [Lec 11, LDA](https://www.sjsu.edu/faculty/guangliang.chen/Math253S20/lec11lda.pdf), explains math behind LDA
* B. Ghojogh: [Eigenvalue and Generalized Eigenvalue Problems: Tutorial](https://arxiv.org/pdf/1903.11240.pdf) (2022)

#### Expectation Maximization
* V. Lavrenko: [Mixture Models](https://www.youtube.com/watch?v=REypj2sy_5U&list=PLBv09BD7ez_4e9LtmK626Evn1ion6ynrt) (2014)
* Andrew Ng, Stanford CS229: [L14: Expectation-Maximization](https://www.youtube.com/watch?v=rVfZHWTwXSA&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=14) (2019)

#### Folding
* [MIT Robotics - David Held - Perceptual Robot Learning](https://www.youtube.com/watch?v=YSj4cIFxvhc) (2021)

#### Distributed Training
* Medium: [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255), Thomas Wolf (2018)
* Towards Data Science: [Distributed Neural Network Training In Pytorch](https://towardsdatascience.com/distributed-neural-network-training-in-pytorch-5e766e2a9e62), Nilesh Vijayrania (2020)
* Serge-Paul Carrasco: [Distributed and Declarative Deep Learning Systems](https://asiliconvalleyinsider.com/2021/09/12/distributed-and-declarative-deep-learning-systems/) (2021)
* [Ludwig: A type-based declarative deep learning toolbox](https://arxiv.org/pdf/1909.07930.pdf)
* Horovod
  * [Docs](https://horovod.readthedocs.io/en/stable/)
  * [Horovod: Multi-GPU and multi-node data parallelism](http://www.idris.fr/eng/jean-zay/gpu/jean-zay-gpu-hvd-tf-multi-eng.html)
  * [Deep Learning at Scale with Horovod feat. Travis Addair](https://www.youtube.com/watch?v=DB7oOZ5hyrE&t=222s), Stanford MLSys Seminar Episode 10 (2021)
* [determined.ai](http://determined.ai)
  * [Why Slurm Makes Deep Learning Engineers Squirm](https://www.determined.ai/blog/slurm-lacking-deep-learning)
  * [Distributed Training using Apache MXNet with Horovod](https://medium.com/apache-mxnet/distributed-training-using-apache-mxnet-with-horovod-44f98bf0e7b7)
  * [AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/horovod-mxnet-distributed-training/): [How to run distributed training using Horovod and MXNet on AWS DL Containers and AWS  Deep Learning AMIs](https://aws.amazon.com/blogs/machine-learning/horovod-mxnet-distributed-training/)
* [DRAGON: A Dynamic Scheduling and Scaling Controller for Managing Distributed Deep Learning Jobs in Kubernetes Cluster](https://www.scitepress.org/Papers/2019/77076/77076.pdf), C. Lin et al (2019)
* [Analysis and Comparison of Distributed Training Techniques for Deep Neural Networks in a Dynamic Environment](http://kth.diva-portal.org/smash/get/diva2:1224181/FULLTEXT01.pdf), E. Gebremeskel (2018)
* [Bringing HPC Techniques to Deep Learning](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/), Andrew Gibiansky (2017)
* [Fast Multi-GPU collectives with NCCL](https://developer.nvidia.com/blog/fast-multi-gpu-collectives-nccl/), Nathan Luehr, NVidia (2016)
* [Fully Sharded Data Parallel: faster AI training with fewer GPUs](https://engineering.fb.com/2021/07/15/open-source/fsdp/), M. Ott et al (2021)

#### Optimization
* [CS231n](http://cs231n.stanford.edu/) [Lecture 7](https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=7&t=13s) (2017)
* [Practical Recommendations for Gradient-Based Training of Deep
Architectures](https://arxiv.org/pdf/1206.5533v2.pdf), Y. Bengio (2012)
* [On the importance of initialization and momentum in deep learning](https://www.cs.toronto.edu/~fritz/absps/momentum.pdf), I. Sutskever et al (2013)
* [Identifying and attacking the saddle point problem in high-dimensional non-convex optimization](https://arxiv.org/pdf/1406.2572.pdf), Y. Dauphin et al (2014)
* [optim.Adam vs optim.SGD. Let’s dive in](https://medium.com/@Biboswan98/optim-adam-vs-optim-sgd-lets-dive-in-8dbf1890fbdc)
* [fast.ai](https://www.fast.ai): [AdamW and Super-convergence is now the fastest way to train neural nets ](https://www.fast.ai/2018/07/02/adam-weight-decay/), by S. Gugger and J. Howard (2018)
* S. Raschka [L12: Learning rates and advanced optimization algorithms](https://sebastianraschka.com/blog/2021/dl-course.html#l12-learning-rates-and-advanced-optimization-algorithms) (2020)
* P. Wirth: [Which Optimizer should I use for my ML Project?](https://www.lightly.ai/post/which-optimizer-should-i-use-for-my-machine-learning-project) (2020)

#### Network tuning
* [How to Avoid Overfitting in Deep Learning Neural Networks](https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/), J. Brownlee (2018)

#### Autoencoders and Variational Autoencoders
* D.P. Kingma, M. Welling [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) (2015)
* Sebastian Raschka: [Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html) (2021)
  * [L16: Autoencoders](https://sebastianraschka.com/blog/2021/dl-course.html#l16-autoencoders)
  * [L17: Variational autoencoders](https://sebastianraschka.com/blog/2021/dl-course.html#l17-variational-autoencoders)
  * [Machine Learning with PyTorch and Scikit-Learn](https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319) Chap. 17
* Valerio Velardo: [The Sound of AI](https://www.youtube.com/@ValerioVelardoTheSoundofAI)
  * [Generating Sound with Neural Networks](https://www.youtube.com/playlist?list=PL-wATfeyAMNpEyENTc-tVH5tfLGKtSWPp)
    * [3 - How do AutoEncoders work?](https://www.youtube.com/watch?v=xwrzh4e8DLs)
    * [9 - From Autoencoders to Variational Autoencoders: Part 1](https://www.youtube.com/watch?v=b8AzCgY1gZI)
    * [10 - From Autoencoders to Variational Autoencoders: Part 2](https://www.youtube.com/watch?v=lRsqFbgGyKg)
    * [11 - Implementing a Variational Autoencoder](https://i.ytimg.com/vi/A6mdOEPGM1E/hqdefault.jpg?sqp=-oaymwEcCNACELwBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLDfOmHXPwsArihUsopN9czV3AZIag) (in Keras)
* A. Geron: [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/14920326461) (Chap 17)
  * [17_autoencoders_gans_and_diffusion_models.ipynb](https://github.com/andrei-radulescu-banu/handson-ml3/blob/main/17_autoencoders_gans_and_diffusion_models.ipynb)
* CS229:
  * [L20 - Variational Autoencoders](https://www.youtube.com/watch?v=-TPFg-RG-KY&list=PLoROMvodv4rNH7qL6-efu_q2_bPuy0adh&index=20) (Summer 2019)
  * [The EM algorithm](http://cs229.stanford.edu/summer2019/cs229-notes8.pdf)
* M2L:
  * [2021 3.1 Variational inference, VAE's and normalizing flows - Rianne van den Berg](https://www.youtube.com/watch?v=-hcxTS5AXW0)
* D. Bei et al: [Variational Inference: A Review for Statisticians](https://arxiv.org/pdf/1601.00670.pdf)
* London Machine Learning Meetup: [Max Welling - Make VAEs Great Again: Unifying VAEs and Flows](https://www.youtube.com/watch?v=bXp8fk4MRXQ) (2020)

#### GANs
* [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/14920326461), by Aurelien Geron (3rd ed, 2022), Chap 17
* S. Reed et al: [Generative Adversarial Text to Image Synthesis](https://arxiv.org/pdf/1605.05396.pdf) (2016)
* T. Karras et al: [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/pdf/1812.04948.pdf) (2018)

#### Diffusion
* [Stable Diffusion: DALL-E 2 For Free, For Everyone!](https://www.youtube.com/watch?v=nVhmFski3vg)
* [Tutorial on diffusion](https://www.youtube.com/watch?v=cS6JQpEY9cs) (3.5 hrs)
* [MIT 6.S192 - Lecture 20: Generative art using diffusion](https://www.youtube.com/watch?v=xYJEvihz3OI), by Prafulla Dhariwal
* M2L:
  * [2022.10 Variational autoencoders and Diffusion Models - Tim Salimans](https://www.youtube.com/watch?v=pea3sH6orMc&t=828s)
* [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/14920326461), by Aurelien Geron (3rd ed, 2022), Chap 17
* [Tutorial on Denoising Diffusion-based Generative Modeling: Foundations and Applications](https://www.youtube.com/watch?v=cS6JQpEY9cs) (3.5 hrs)
* [MIT 6.S192 - Lecture 20: Generative art using diffusion](https://www.youtube.com/watch?v=xYJEvihz3OI), Prafulla Dhariwal, OpenAI
* [Stable Diffusion: DALL-E 2 For Free, For Everyone!](https://www.youtube.com/watch?v=nVhmFski3vg) (2022)
* Huggingface: [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion)
* OpenAI's A. Nichols et al: [Point·E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/pdf/2212.08751.pdf) (2022)

#### Text to image
* Yannic Kilcher: [OpenAI CLIP: ConnectingText and Images (Paper Explained)](https://www.youtube.com/watch?v=T9XSU0pKX2E) (2022)

#### Convolutional Nets
* [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567v1.pdf), C. Szegedy el at (2015)
* Blog: [Speeding up Convolutional Neural Networks](https://towardsdatascience.com/speeding-up-convolutional-neural-networks-240beac5e30f), [Alex Burlacu](https://alexandruburlacu.github.io/) (2018)
* M. Tang et al: [EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/pdf/2104.00298.pdf) (2021), [github](https://github.com/d-li14/efficientnetv2.pytorch)

#### Image style transfer
* [Texture Synthesis Using Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2015/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf), L.A.Gatys et al (2016) [code](https://github.com/leongatys/DeepTextures)
* [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf), L.A.Gatys et al (2016), [torch models by J.C.Johnson](https://github.com/jcjohnson/neural-style)
* [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/pdf/1603.08155.pdf), J. Johnson et al (2016)

#### Object Detection from Point Clouds
* PapersWithCode: [PointPillars](https://paperswithcode.com/search?q_meta=&q_type=&q=pointpillars)
  * [PointPillars: Fast Encoders for Object Detection from Point Clouds](https://arxiv.org/pdf/1812.05784v2.pdf), Alex H. Lang et al (2019)
  * [Optimisation of the PointPillars network for 3D object detection in point clouds](https://arxiv.org/pdf/2007.00493v1.pdf), J. Stanisz et al (2020)
* Y. Guo et al: [Deep Learning for 3D Point Clouds: A Survey](https://arxiv.org/pdf/1912.12033.pdf) (2020)

#### Object Detection from Images
* [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640v5.pdf), J. Redmon et al (2016), [http://pjreddie.com/yolo/](http://pjreddie.com/yolo/), Y Liao et al (2021)

#### Geometric Deep Learning
* Michael Bronstein: [Geometric Deep Learning: the Erlangen Programme of ML](https://iclr.cc/virtual/2021/invited-talk/3717) (2021)
  * [Geometric Deep Learning: Grids, Groups, Graphs,Geodesics, and Gauges](https://arxiv.org/pdf/2104.13478.pdf), M. Bronstein et al (2021)
  * ML Street Talk #60: [Geometric Deep Learning Blueprint](https://www.youtube.com/watch?v=bIZB1hIJ4u8) (2021)
* Max Welling
  * ML Street Talk #36: [Max Welling: Quantum, Manifolds & Symmetries in ML](https://www.youtube.com/watch?v=mmDw5glry9w) (2021)
  * IAS Seminar on Theoretical ML: [Graph Nets: The Next Generation](https://www.youtube.com/watch?v=Wx8J-Kw3fTA) (2020)
*  Machine Learning Street Talk #75: [Emergence with Danielle Grattarola](https://www.youtube.com/watch?v=MDt2e8XtUcA) (2022)

#### Reinforcement Learning
* R. Sutton, A. Barto: [Reinforcement Learning, second edition: An Introduction](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=d_pd_sim_sccl_3_1/146-1943148-1230166) (2018)

#### Chat Agents
* L. Ouyang el al: [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf) (2022)
* TalkRL: [John Schulman](https://share.transistor.fm/s/2bfa4dc4) interview (2022)

### AGI
* [AIXI](https://en.wikipedia.org/wiki/AIXI)
  * Marcus Hutter: [Universal Artificial Intelligence: Sequential Decisions Based On Algorithmic Probability](https://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) (2005)
  * Shane Legg: [Machine Super Intelligence](https://www.vetta.org/documents/Machine_Super_Intelligence.pdf) (2008)
    * AI Channel: [DeepMind's Shane Legg - Machine Super Intelligence](https://www.youtube.com/watch?v=tFgJHzliy94) (2009)
* J. Bach: [When Artificial Intelligence Becomes General Enough to Understand Itself. Commentary on Pei Wang’s paper "On Defining Artificial Intelligence"](https://d1wqtxts1xzle7.cloudfront.net/67233992/article-p1-with-cover-page-v2.pdf?Expires=1670126983&Signature=bVqu55Xl9b13VpZRfyOqYGjxrsDJ84oXp~Z0I6lonMLqGvWwwJR9sdiKbduNukJfzGCRpP75B8gDUbIGJhKo2hvIAITAB6VqP6ztmcM7volamlqPkDJofWC9Wq0CtHW9I3wK2SCBIaUFhd43-5C8MHyrDQcGYORthNi0PPN2P8wn3uERQzPjYVvH1igKoM1lfn7ok8MVd8eynzo5G1bh1x9fBXgSH59oRMH58HHu8bsDTLPMex~l0O1j2-fKsD0OMt8ckHlBLJgDunYWryqm63DuZGaSdzQ2mTWxwiOogSDe4GmRPG7Hpg4HaqboYKT3OTp6BtVGmTKIpuaCPNFTyw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA#page=16) (2020)
 * A. Franz et al: [A theory of incremental compression](https://arxiv.org/pdf/1908.03781.pdf) (2020)
* Y. LeCun: [A Path Towards Autonomous Machine Intelligence](https://openreview.net/pdf?id=BZ5a1r-kVsf), draft (2022), [tweet](https://twitter.com/ylecun/status/1541493132168249344)
* Silver, Singh, Sutton: [Reward is enough](https://www.deepmind.com/publications/reward-is-enough) (2021)
* D. Ha, J. Schmidthuber: [World Models](https://arxiv.org/pdf/1803.10122.pdf) (2018)

#### Causality
* [Causality for Machine Learning](https://arxiv.org/pdf/1911.10500.pdf), Bernhard Schölkopf (2019)
* Y. Bengio talk: [Deep Learning Cognition](https://www.youtube.com/watch?v=GibjI5FoZsE) (2020)

#### Natural Language Processing (NLP)
* torchtext [Release Notes](https://github.com/pytorch/text/releases), [examples](https://github.com/pytorch/text/tree/master/examples)
* Tutorial: [Migrate torchtext from the legacy API to the new API](https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb)
* J. Geiping, T. Goldstein: [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034) (2022), [github](https://github.com/JonasGeiping/cramming), [review by Lucas Beyer](https://twitter.com/giffmana/status/1608568387583737856)

#### RNN
* Towards Data Science: [Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45), by R. Karim (2018)
* Towards Data Science: [Counting No. of Parameters in Deep Learning Models by Hand](https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889), by R. Karim (2019)

#### Attention, Transformers
* MIT 6.S191: [Recurrent Neural Networks and Transformers](https://www.youtube.com/watch?v=QvkQ1B3FBqA&t=1721s) (2022)
* Leo Dirac: [LSTM is dead. Long Live Transformers!](https://www.youtube.com/watch?v=S27pHKBEp30) (2019)
* Sebastian Raschka: [L19.5.1 The Transformer Architecture](https://www.youtube.com/watch?v=tstbZXNCfLY)
* Towards Data Science: [How to code The Transformer in Pytorch](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec), by S. Lynn-Evans (2018)
* Lucas Beyer: [Transformers](https://www.youtube.com/watch?v=UpfcyzoZ644), Mediterranean ML Summer School 2022 seminar
* [Lil'Log](https://lilianweng.github.io): [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/) (2023)
* Papers
  * J. von Oswald et al: [Transformers learn in-context by gradient descent](https://arxiv.org/pdf/2212.07677.pdf) (2022)
  * R. Pope et al: [Efficiently scaling transformer inference](https://arxiv.org/pdf/2211.05102.pdf) (2022)

#### Language Models
* [Self-supervised learning: The dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) (2021)

#### Transformers and Lidars
* [Transformers in 3D Point Clouds: A Survey](https://arxiv.org/pdf/2205.07417.pdf) (2022)

#### Energy Based Models
* [A Tutorial on Energy-Based Learning](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf), Y. LeCun et al (2006)
* Y. LeCun: [Energy-Based Self-Supervised Learning](https://www.youtube.com/watch?v=A7AnCvYDQrU), IPAM (2019), [slides](http://helper.ipam.ucla.edu/publications/mlpws4/mlpws4_15927.pdf)
* [The Physics of Energy-Based Models](https://physicsofebm.github.io/), P. Huembeli et al (2021)
* A. Dawid et al: [Modern applications of machine learning in quantum sciences](https://arxiv.org/ftp/arxiv/papers/2204/2204.04198.pdf) (2022)
* M.A. Carreira-Perpinan, G.E. Hinton: [On Contrastive Divergence Learning](https://www.cs.toronto.edu/~hinton/absps/cdmiguel.pdf) (2005)
* B.A. Cipra: [An Introduction to the Ising Model](https://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/isingIntro.pdf) (1987), AMM Monthly. Finally I can understand what the Ising Model is about.
* E. Aurell, M. Ekberg: [Inverse Ising inference using all the data](https://arxiv.org/pdf/1107.3536.pdf) (2012)

#### Dataset Pruning
* Surya Ganguli: [Statistical mechanics of neural networks](https://www.youtube.com/watch?v=D_TyXVzD3hs) (2022), 2nd part

#### Contrastive Learning
* G. Hinton: [The Forward-Forward Algorithm: Some Preliminary Investigations](https://www.cs.toronto.edu/~hinton/FFA13.pdf), [talk](https://nips.cc/virtual/2022/invited-talk/55869) (2022)

#### News Ranking
* X. Ni et al: [Prioritizing Original News on Facebook](https://arxiv.org/pdf/2102.08465.pdf) (2021)

#### Identifying Harmful Content
* Facebook: [Harmful content can evolve quickly. Our new AI system adapts to tackle it.](https://ai.facebook.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it/) (2021)
  * S. Wang et al: [Entailment as Few-Shot Learner](https://arxiv.org/pdf/2104.14690.pdf?fbclid=IwAR2ImbTwKdt1thb0pfGVYxQ0D9EgjotlhfOVBeDNyt6Gnn8j7TNwHVve1FA) (2021)
* Facebook: [How AI is getting better at detecting hate speech](https://ai.facebook.com/blog/how-ai-is-getting-better-at-detecting-hate-speech/) (2020)

#### Large Model Training
* [Lil'Log](https://lilianweng.github.io): [How to Train Really Large Models on Many GPUs?](https://lilianweng.github.io/posts/2021-09-25-train-large/) (2021)
* Lilian Weng, Greg Brockman: [Techniques for Training Large Neural Networks](https://openai.com/blog/techniques-for-training-large-neural-networks/) (2022)

#### Articles
* [How neural networks learn from experience](http://www.cs.toronto.edu/~hinton/absps/sciam92.pdf), G. Hinton (1992)
* [Neural networks and physical systems with emergent collective computational abilities](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/pdf/pnas00447-0135.pdf), J. J. Hopfield (1982)
* [Reducing the Dimensionality of Data with Neural Networks](https://www.cs.toronto.edu/~hinton/science.pdf), G. E. Hinton and R. R. Salakhutdinov (2006), using Bolzmann machines to initialize weights close to a good solution
* [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton (2012), describes the AlexNet Conv network.
* [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/pdf/1703.03400.pdf), C. Finn, P. Abbeel, S. Levine (2017)
  * S. Khodadadeh: [Model Agnostic Meta Learning](https://www.youtube.com/watch?v=wT45v8sIMDM) (2018)
  * Deep Learning Explainer: [Toward Efficient Learning: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://www.youtube.com/watch?v=tGTNplKgt6Q) (2020)
* [Meta-Learning with Implicit Gradients](https://arxiv.org/pdf/1909.04630.pdf), A. Rajeswaran et al (2019), [video](https://www.youtube.com/watch?v=u5BkO8XMS2I)
* [The Mechanics of n-Player Differentiable Games](https://arxiv.org/pdf/1802.05642.pdf), D. Balduzzi et al (2018)
* [Simple, Distributed, and AcceleratedProbabilistic Programming](https://arxiv.org/pdf/1811.02091.pdf), D. Tran et al (2018)
* [Machine Theory of Mind](https://arxiv.org/pdf/1802.07740.pdf), N.C. Rabinowitz et al (2018)
* [Recent Advances in Deep Learning for Object Detection](https://arxiv.org/pdf/1908.03673.pdf), X. Wu (2019)
* [Online Bayesian Goal Inference for Boundedly-Rational Planning Agents](https://arxiv.org/pdf/2006.07532.pdf), T. Zhi-Xuan et al (2020)
* [Open Problems in Cooperative AI](https://arxiv.org/pdf/2012.08630.pdf), A. Dafoe et al (2020)
* [Rethinking the maturity of artificial intelligence in safety-critical settings](http://hal.pratt.duke.edu/sites/hal.pratt.duke.edu/files/u36/reality%20check%20final_compressed.pdf), M.L. Cummings, 2019
* Sendtex tutorials at https://pythonprogramming.net
* [Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis](https://arxiv.org/pdf/1802.09941.pdf), Bel-Nun, Hoefler (2018), [video](https://www.youtube.com/watch?v=xtxxLWZznBI)
* Medium: [RegNet or How to methodologically design effective networks](https://medium.com/analytics-vidhya/regnet-or-how-to-methodologically-design-effective-networks-c3560c1cf436), Chris Ha (2000)
* [Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users](https://arxiv.org/pdf/2007.06823.pdf), L. V. Jospin et al (2021)
* R. Ghugare et al: [Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective](https://arxiv.org/abs/2209.08466) (2022), [talk](https://www.youtube.com/watch?v=pgaULpPhzus), [code](https://alignedlatentmodels.github.io)
* J. Tenenbaum et al: [3DP3: 3D Scene Perception via Probabilistic Programming](https://arxiv.org/pdf/2111.00312.pdf) (2021)
* [Why do tree-based models still outperform deep learning on tabular data?](https://arxiv.org/pdf/2207.08815.pdf), Léo Grinsztajn et al (2022)

#### Posts
* [Colah's blog](http://colah.github.io/)
* [distill.pub](https://distill.pub/)
* [MuZero](https://en.wikipedia.org/wiki/MuZero) and [The Evolution of AlphaGo to MuZero](https://towardsdatascience.com/the-evolution-of-alphago-to-muzero-c2c37306bf9)
* [McGill COMP-424 Intro to AI Lecture Notes](https://www.cs.mcgill.ca/~dprecup/courses/AI/Lectures) (Doina Precup, 2013), [Lecture 16, Why does a finite MDP optimal policy exist?](https://www.cs.mcgill.ca/~dprecup/courses/AI/Lectures/ai-lecture16.pdf)
* Open AI: [Safety Gym] (https://openai.com/blog/safety-gym/) (2019)
* [OpenAI Baselines](https://github.com/openai/baselines/), a set of high-quality implementations of reinforcement learning algorithms
* [ConvnetJS](https://cs.stanford.edu/people/karpathy/convnetjs/) demo: [Toy 2d classification with 2-layer neural network](https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html), A. Karpathy
* Adrian Rosebrock [tutorials](https://www.pyimagesearch.com/author/adrian/)
* The [Important Definitions](https://github.com/rafaelpadilla/Object-Detection-Metrics) of Rafael Padilla repo is a very good introduction to the relationship between IOU, precision/recall, PR curve, average precision
* [Ben Dickson](https://bdtechtalks.com/author/bendee983/): [The challenges of applied machine learning](https://bdtechtalks.com/2021/04/19/applied-machine-learning-challenges/) (2021)
* Jonathan Hui: [How to start a Deep Learning project?](https://jonathan-hui.medium.com/how-to-start-a-deep-learning-project-d9e1db90fa72) (2018)
* Georgii Evtushenko: [Multi-GPU Programming](https://medium.com/gpgpu/multi-gpu-programming-6768eeb42e2c)
* Mihail Eric: [MLOps Is a Mess But That's to be Expected](https://www.mihaileric.com/posts/mlops-is-a-mess/) (2022)
* Matt Turk: [Red Hot: The 2021 Machine Learning, AI and Data (MAD) Landscape](https://mattturck.com/data2021/)
* A. Kumar, I. Kostrikov, S. Levine: [Should I Use Offline RL or Imitation Learning?](https://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/) (2022)
* Giuliano Giacaglia: [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591) (2019)
* Sebastian Raschka [Ahead of AI](https://www.linkedin.com/newsletters/ahead-of-ai-6994407435943772160/) (2022)
  * [#1: A Diffusion of Innovations](https://www.linkedin.com/pulse/ahead-ai-1-diffusion-innovations-sebastian-raschka-phd/)
  * [#2 - Transformers, Fast and Slow](https://www.linkedin.com/pulse/ahead-ai-2-transformers-fast-slow-sebastian-raschka-phd/?trackingId=h2oFDMetQ%2FyynqBmG4FUzQ%3D%3D)

#### Web sites
* [Papers with Code](https://paperswithcode.com/)

#### MLOps
* D. Sculley et al: [Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf) (2015)
* Berkeley: Full Stack Deep Learning: [Lecture 6: MLOps Infrastructure & Tooling](https://fullstackdeeplearning.com/spring2021/lecture-6/)

#### People
* [Justin Johnson](https://web.eecs.umich.edu/~justincj/)

#### Other
* [Artificial Intelligence](artificial_intelligence.md)
* [Cloud Data Platform](cloud_data_platform.md)
* [Cognitive Science](cognitive_science.md)
* [Computation Theory](computation_theory.md)
* [GPUs](gpus.md)
* [Information Theory](information_theory.md)
* [Meta Learning](meta_learning.md)
* [MLOps](mlops.md)
* [Natural Language Processing](natural_language_processing.md)
* [Probabilities and Statistics](probabilities_and_statistics.md)
* [Robotics](robotics.md)
* [Self Driving Cars](self_driving_cars.md)
* [Computational Topology](computational_topology.md)
